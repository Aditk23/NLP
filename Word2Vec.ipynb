{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLDOeCKoChBO"
      },
      "source": [
        "##Word2Vec (Word To Vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOCmF2C1mP5T"
      },
      "source": [
        "Image and audio processing systems work with rich, high-dimensional datasets encoded as vectors of the individual raw pixel-intensities for image data, thus all the information is encoded in the data hence the relation between various entities in the system like (cats and dogs) can be established.\n",
        "\n",
        "But, when it comes to natural language processing systems traditionally it treats words as discrete atomic symbols, and therefore ‘cat’ may be represented as Id537 and ‘dog’ as Id143.These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about ‘cats’ when it is processing data about ‘dogs’ (such that they are both animals, four-legged, pets, etc).\n",
        "\n",
        "Representing words as unique, discrete ids furthermore leads to data sparsity, and usually means that we may need more data in order to successfully train statistical models. Using vector representations can overcome some of these obstacles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKrTXTMA9oX0"
      },
      "source": [
        "Word2vec’s applications extend beyond parsing sentences in the wild. It can be applied just as well to genes, code, likes, playlists, social media graphs and other verbal or symbolic series in which patterns may be discerned.\n",
        "\n",
        "The purpose and usefulness of Word2vec is to group the vectors of similar words together in vectorspace. That is, it detects similarities mathematically. Word2vec creates vectors that are distributed numerical representations of word features, features such as the context of individual words. It does so without human intervention.\n",
        "\n",
        "Given enough data, usage and contexts, Word2vec can make highly accurate guesses about a word’s meaning based on past appearances. Those guesses can be used to establish a word’s association with other words (e.g. “man” is to “boy” what “woman” is to “girl”), or cluster documents and classify them by topic. Those clusters can form the basis of search, sentiment analysis and recommendations in such diverse fields as scientific research, legal discovery, e-commerce and customer relationship management.\n",
        "\n",
        "The output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SjQ4-tk-j5g"
      },
      "source": [
        "Word2vec is similar to an autoencoder, encoding each word in a vector, but rather than training against the input words through reconstruction, as a restricted Boltzmann machine does, word2vec trains words against other words that neighbor them in the input corpus.\n",
        "\n",
        "It does so in one of two ways, either using context to predict a target word (a method known as continuous bag of words, or CBOW), or using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets\n",
        "![alt text](https://skymind.ai/images/wiki/word2vec_diagrams.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFCdc-O8_Isz"
      },
      "source": [
        "**Skip-gram Model**\n",
        " \n",
        "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word). Considering our simple sentence from earlier, “the quick brown fox jumps over the lazy dog”. If we used the CBOW model, we get pairs of (context_window, target_word)where if we consider a context window of size 2, we have examples like ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so on. Now considering that the skip-gram model’s aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context [quick, fox] given target word ‘brown’ or [the, brown] given target word ‘quick’ and so on. Thus the model tries to predict the context_window words based on the target_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrVCp9N8WuyC"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/797/1*yiH5sZI-IBxDSQMKhvbcHw.png/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiVJ-izjRjXh"
      },
      "source": [
        "### Implementing the Skip-gram Model\n",
        " \n",
        "Let’s now try and implement this model from scratch to gain some perspective on how things work behind the scenes and also so that we can compare it with our implementation of the CBOW model. The implementation will focus on five parts\n",
        "\n",
        "* Build the corpus vocabulary\n",
        "* Build a skip-gram [(target, context), relevancy] generator\n",
        "* Build the skip-gram model architecture\n",
        "* Train the Model\n",
        "* Get Word Embeddings\n",
        "\n",
        "\n",
        "\n",
        "**Build the corpus vocabulary**\n",
        "\n",
        "To start off, we will follow the standard process of building our corpus vocabulary where we extract out each unique word from our vocabulary and assign a unique identifier, similar to what we did in the CBOW model. We also maintain mappings to transform words to their unique identifiers and vice-versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bO7wt4kTQdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bacc7c2-634e-4f45-d606-1181062a30bd"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRhN9yhNZIDR"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/968/1*cnzY08TWRxG3lMKExbslHw.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZGYnaw0UFDM"
      },
      "source": [
        "We have a pair of input words for each training example consisting of one input target word having a unique numeric identifier and one context word having a unique numeric identifier. If it is a positive sample the word has contextual meaning, is a context wordand our label Y=1, else if it is a negative sample, the word has no contextual meaning, is just a random word and our label Y=0. We will pass each of them to an embedding layer of their own, having size (vocab_size x embed_size) which will give us dense word embeddings for each of these two words (1 x embed_size for each word). Next up we use a merge layer to compute the dot product of these two embeddings and get the dot product value. This is then sent to the dense sigmoid layer which outputs either a 1 or 0. We compare this with the actual label Y (1 or 0), compute the loss, backpropagate the errors to adjust the weights (in the embedding layer) and repeat this process for all (target, context) pairs for multiple epochs. The following figure tries to explain the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YHJFI3eflYd"
      },
      "source": [
        "![alt text](https://cdn-images-1.medium.com/max/800/1*4Uil1zWWF5-jlt-FnRJgAQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlfBbMiIXiqY"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "corpus_raw = 'He is the king . The king is royal . She is the royal  queen '\n",
        "\n",
        "# convert to lower case\n",
        "corpus_raw = corpus_raw.lower()\n",
        "\n",
        "words = []\n",
        "for word in corpus_raw.split():\n",
        "    if word != '.': # because we don't want to treat . as a word\n",
        "        words.append(word)\n",
        "\n",
        "words = set(words) # so that all duplicate words are removed\n",
        "word2int = {}\n",
        "int2word = {}\n",
        "vocab_size = len(words) # gives the total number of unique words\n",
        "\n",
        "for i,word in enumerate(words):\n",
        "    word2int[word] = i\n",
        "    int2word[i] = word\n",
        "\n",
        "# raw sentences is a list of sentences.\n",
        "raw_sentences = corpus_raw.split('.')\n",
        "sentences = []\n",
        "for sentence in raw_sentences:\n",
        "    sentences.append(sentence.split())\n",
        "\n",
        "WINDOW_SIZE = 2\n",
        "\n",
        "data = []\n",
        "for sentence in sentences:\n",
        "    for word_index, word in enumerate(sentence):\n",
        "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
        "            if nb_word != word:\n",
        "                data.append([word, nb_word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oENXqmrmXX9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "3db28613-b945-44b3-fca3-fa96cf81b47c"
      },
      "source": [
        "# function to convert numbers to one hot vectors\n",
        "def to_one_hot(data_point_index, vocab_size):\n",
        "    temp = np.zeros(vocab_size)\n",
        "    temp[data_point_index] = 1\n",
        "    return temp\n",
        "\n",
        "x_train = [] # input word\n",
        "y_train = [] # output word\n",
        "\n",
        "for data_word in data:\n",
        "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
        "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))\n",
        "\n",
        "# convert them to numpy arrays\n",
        "x_train = np.asarray(x_train)\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "# making placeholders for x_train and y_train\n",
        "x = tf.Variable(0,shape=(None, vocab_size))\n",
        "y_label = tf.Variable(0,shape=(None, vocab_size))\n",
        "\n",
        "EMBEDDING_DIM = 5 # you can choose your own number\n",
        "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
        "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM])) #bias\n",
        "hidden_representation = tf.add(tf.matmul(x,W1), b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
        "b2 = tf.Variable(tf.random_normal([vocab_size]))\n",
        "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))\n",
        "\n",
        "\n",
        "sess = tf.Session()\n",
        "init = tf.global_variables_initializer()\n",
        "sess.run(init) #make sure you do this!\n",
        "\n",
        "# define the loss function:\n",
        "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))\n",
        "\n",
        "# define the training step:\n",
        "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy_loss)\n",
        "\n",
        "n_iters = 10000\n",
        "# train for n_iter iterations\n",
        "\n",
        "for _ in range(n_iters):\n",
        "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
        "    print('loss is : ', sess.run(cross_entropy_loss, feed_dict={x: x_train, y_label: y_train}))\n",
        "\n",
        "vectors = sess.run(W1 + b1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-72f9beb40932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# making placeholders for x_train and y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v2_call\u001b[0;34m(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kws)\u001b[0m\n\u001b[1;32m    235\u001b[0m                         shape=None):\n\u001b[1;32m    236\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator_v2\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2665\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m       shape=shape)\n\u001b[0m\u001b[1;32m   2668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1583\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1723\u001b[0m                   \u001b[0;34m\"The initial value's shape (%s) is not compatible with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m                   \u001b[0;34m\"the explicitly supplied `shape` argument (%s).\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m                   (initial_value.shape, shape))\n\u001b[0m\u001b[1;32m   1726\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m             \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The initial value's shape (()) is not compatible with the explicitly supplied `shape` argument ((None, 7))."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN_sV-NrXrtm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "outputId": "7b2058d5-aafc-47f2-b41e-098527346664"
      },
      "source": [
        "def euclidean_dist(vec1, vec2):\n",
        "    return np.sqrt(np.sum((vec1-vec2)**2))\n",
        "\n",
        "def find_closest(word_index, vectors):\n",
        "    min_dist = 10000 # to act like positive infinity\n",
        "    min_index = -1\n",
        "    query_vector = vectors[word_index]\n",
        "    for index, vector in enumerate(vectors):\n",
        "        if euclidean_dist(vector, query_vector) < min_dist and not np.array_equal(vector, query_vector):\n",
        "            min_dist = euclidean_dist(vector, query_vector)\n",
        "            min_index = index\n",
        "    return min_index\n",
        "\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "model = TSNE(n_components=2, random_state=0)\n",
        "np.set_printoptions(suppress=True)\n",
        "vectors = model.fit_transform(vectors) \n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "normalizer = preprocessing.Normalizer()\n",
        "vectors =  normalizer.fit_transform(vectors, 'l2')\n",
        "\n",
        "print(vectors)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "print(words)\n",
        "for word in words:\n",
        "    print(word, vectors[word2int[word]][1])\n",
        "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.7752489   0.63165593]\n",
            " [-0.54884887 -0.8359216 ]\n",
            " [ 0.9997878  -0.02059868]\n",
            " [ 0.7905123  -0.6124462 ]\n",
            " [ 0.25224355 -0.96766376]\n",
            " [ 0.8785238  -0.47769862]\n",
            " [-0.36656952  0.93039066]]\n",
            "{'royal', 'the', 'queen', 'she', 'he', 'is', 'king'}\n",
            "royal 0.63165593\n",
            "the -0.8359216\n",
            "queen -0.02059868\n",
            "she -0.6124462\n",
            "he -0.96766376\n",
            "is -0.47769862\n",
            "king 0.93039066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAHACAYAAAC1YPKRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ne4IGAGhP0AAA7QkaAKA9QQMAtPf/Aa7zG5w+myNeAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTFdbLvKX8ke"
      },
      "source": [
        "Of course the accuracy of such system is not be trusted. In real use case we are gonna use pre trained models with gensim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJW7ZcWbk6yT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3a5b31-1d8e-496a-dcec-0135a7103519"
      },
      "source": [
        "! pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTwJg-PHk2xe"
      },
      "source": [
        "### Word embeddings through gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lnGaSNKlMxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ff3726-fdb2-4341-ce8b-6c2af9f7fab8"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# summarize the loaded model\n",
        "print(model)\n",
        "# summarize vocabulary\n",
        "words = list(model.wv.vocab)\n",
        "print(words)\n",
        "# access vector for one word\n",
        "print(model['final'])\n",
        "# save model\n",
        "model.save('model.bin')\n",
        "# load model\n",
        "new_model = Word2Vec.load('model.bin')\n",
        "print(new_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n",
            "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
            "[-3.7835222e-03  2.0042905e-03  2.2990281e-04 -1.2208561e-03\n",
            " -4.3613871e-04 -3.9045967e-03  1.8054344e-03  2.5673355e-03\n",
            " -4.6929019e-03 -4.1167387e-03  1.2083285e-03 -2.6660601e-03\n",
            " -4.8384257e-03 -2.0087021e-03  4.9847050e-04  4.3706466e-03\n",
            "  1.2724249e-03  4.6071722e-03  3.7176907e-04 -4.7008391e-03\n",
            "  4.1795443e-03 -4.1248244e-03  4.7084494e-03  2.6264584e-03\n",
            " -3.7582852e-03 -1.3030982e-03 -2.9409081e-03  3.2188206e-03\n",
            " -1.7577367e-03 -1.7908795e-03 -3.8229092e-03  3.9482652e-03\n",
            " -3.1740188e-03 -1.5365714e-04  1.4529527e-03 -1.8598282e-03\n",
            " -4.5641270e-03  9.7837776e-04 -3.4167115e-03 -3.6915860e-04\n",
            "  3.5106556e-03 -3.1095644e-04  7.5340807e-04  2.1802206e-03\n",
            " -1.2776522e-03 -6.2929542e-04  2.6486928e-03 -1.0675847e-03\n",
            " -1.2619243e-03  2.9710548e-03  2.8953357e-03 -1.1400762e-03\n",
            "  3.1337931e-03  3.5828641e-03  1.8869299e-03  1.5197267e-03\n",
            "  2.5486189e-03 -2.1220362e-03 -2.1779344e-03 -6.9086306e-04\n",
            "  3.0754867e-03 -3.2613596e-03  2.0912634e-03 -3.6109337e-03\n",
            " -4.0964182e-03  6.7540410e-04 -1.6788365e-03 -2.6621339e-03\n",
            "  2.8332020e-03 -3.3433316e-03 -3.1154088e-03  2.5402657e-03\n",
            "  2.6350734e-03  3.6856076e-05  2.7846843e-06 -2.3055088e-03\n",
            "  3.9796517e-03 -2.8690768e-03 -1.2234914e-04 -3.3229834e-03\n",
            "  3.2539866e-03 -3.2775062e-03 -4.8210449e-03  7.8011962e-04\n",
            "  4.4475626e-03 -4.8518206e-05  1.2770053e-03 -2.0406477e-03\n",
            " -4.2582424e-03  2.3118306e-03 -1.1187103e-03 -1.0502497e-03\n",
            "  4.1120844e-03  4.1744770e-03  4.2658821e-03 -2.9269196e-03\n",
            " -4.7012111e-03  3.1034439e-03 -4.0127384e-04  4.3668607e-03]\n",
            "Word2Vec(vocab=14, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  app.launch_new_instance()\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKOQAefC7hvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6801809d-4fbc-480b-e061-937bb6eb4382"
      },
      "source": [
        "print(new_model[\"first\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 2.5503116e-03  2.6400078e-03 -2.4497060e-03 -5.3572422e-04\n",
            " -2.0325491e-03  1.5541700e-03 -4.7720405e-03 -2.7276126e-03\n",
            " -4.9316864e-03  2.6270316e-03  6.5080414e-04  3.7766586e-03\n",
            "  2.9760334e-03 -9.6039858e-04 -4.8139780e-03  2.2456320e-03\n",
            " -1.9232357e-03  3.4962061e-03  3.1506659e-03 -3.6613856e-04\n",
            " -4.5635967e-04 -1.5396453e-03  4.0826248e-03  4.4814050e-03\n",
            " -4.7537172e-03  1.5954729e-03 -1.1576893e-03  3.2201568e-03\n",
            "  1.9953232e-03  3.1332471e-04  3.7847918e-03  2.9944268e-03\n",
            "  2.9732906e-03 -4.9394788e-03  3.7500006e-03 -4.9902298e-03\n",
            "  1.1157364e-03 -3.4418805e-03  1.4733846e-04 -3.8336741e-03\n",
            "  8.1489276e-04 -7.1118376e-04  1.6997654e-04  4.6202186e-03\n",
            " -4.2982437e-03 -7.8105228e-04  9.0875692e-04  1.2465660e-03\n",
            " -7.6181738e-04 -2.6440229e-03  2.9233512e-03 -4.9969540e-03\n",
            "  2.4358269e-03 -1.9439070e-03 -1.2246319e-03  4.3208352e-03\n",
            "  2.5429862e-04  4.6977980e-04  2.0287214e-03  1.4854773e-03\n",
            "  4.3482394e-03  1.6895226e-03 -3.2056300e-03  4.3232176e-03\n",
            "  4.3138820e-03  1.9140254e-03  3.0396013e-03  3.6236360e-03\n",
            "  4.4882111e-03  4.5624189e-03 -2.5342165e-03 -2.9533159e-03\n",
            "  2.2555816e-03 -7.6547801e-04  1.2813899e-04 -2.3164307e-03\n",
            "  9.8988996e-05  4.4842311e-03 -3.6362576e-04  7.5704366e-04\n",
            "  4.4332370e-03  2.5184904e-03 -1.7649571e-04 -4.9201874e-03\n",
            "  7.5589499e-04  4.5851851e-03 -3.3102178e-03 -4.0390468e-03\n",
            " -1.3229608e-03 -3.5364188e-03 -1.8193368e-03 -1.2718093e-03\n",
            "  3.2068619e-03 -2.3660632e-03 -2.9045639e-03 -1.4503188e-03\n",
            " -4.7978782e-03  4.7796345e-03  1.6915024e-03  3.9611910e-03]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjjgeR2_lfCg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "99cadafd-bfac-4e4c-daa6-ca6caa9cd44e"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "# define training data\n",
        "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
        "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
        "\t\t\t['yet', 'another', 'sentence'],\n",
        "\t\t\t['one', 'more', 'sentence'],\n",
        "\t\t\t['and', 'the', 'final', 'sentence']]\n",
        "# train model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "# fit a 2d PCA model to the vectors\n",
        "X = model[model.wv.vocab]\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "# create a scatter plot of the projection\n",
        "pyplot.scatter(result[:, 0], result[:, 1])\n",
        "words = list(model.wv.vocab)\n",
        "for i, word in enumerate(words):\n",
        "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyvGxTmamxgo"
      },
      "source": [
        "Training your own word vectors may be the best approach for a given NLP problem.\n",
        "\n",
        "But it can take a long time, a fast computer with a lot of RAM and disk space, and perhaps some expertise in finessing the input data and training algorithm.\n",
        "\n",
        "An alternative is to simply use an existing pre-trained word embedding.\n",
        "\n",
        "Along with the paper and code for word2vec, Google also published a pre-trained word2vec model on the Word2Vec Google Code Project.\n",
        "\n",
        "A pre-trained model is nothing more than a file containing tokens and their associated word vectors. The pre-trained Google word2vec model was trained on Google news data (about 100 billion words); it contains 3 million words and phrases and was fit using 300-dimensional word vectors.\n",
        "\n",
        "It is a 1.53 Gigabytes file. You can download it from here:\n",
        "\n",
        "[GoogleNews-vectors-negative300.bin.gz](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n",
        "\n",
        "Unzipped, the binary file (GoogleNews-vectors-negative300.bin) is 3.4 Gigabytes.\n",
        "\n",
        "The Gensim library provides tools to load this file. Specifically, you can call the KeyedVectors.load_word2vec_format() function to load this model into memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDnd9mfKYuGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2df57cb-9d0e-4439-aaed-2ad350749891"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ72vRojm5Ey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "5d43335f-fdac-45fc-b990-7f2d0afa0b5d"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "filename = 'GoogleNews-vectors-negative300.bin' # need to have this file locally\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
        "# calculate: (king - man) + woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-12fa995e7e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GoogleNews-vectors-negative300.bin'\u001b[0m \u001b[0;31m# need to have this file locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m     )\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, ignore_ext, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews-vectors-negative300.bin'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxU8YT1HTr8Z"
      },
      "source": [
        "### GloVe\n",
        "\n",
        "Stanford researchers also have their own word embedding algorithm like word2vec called Global Vectors for Word Representation, or GloVe for short.\n",
        "\n",
        "Like word2vec, the GloVe researchers also provide pre-trained word vectors, in this case, a great selection to choose from.\n",
        "\n",
        "You can download the GloVe pre-trained word vectors and load them easily with gensim.\n",
        "\n",
        "The first step is to convert the GloVe file format to the word2vec file format. The only difference is the addition of a small header line. This can be done by calling the glove2word2vec() function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nboKb6RDXEla"
      },
      "source": [
        "You can download the smallest GloVe pre-trained model from the GloVe website. It an 822 Megabyte zip file with 4 different models (50, 100, 200 and 300-dimensional vectors) trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary.\n",
        "\n",
        "The direct download link is here:\n",
        "\n",
        "[glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bKYlXaiW6iZ"
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "glove_input_file = 'glove.6B.100d.txt' # need to have this file locally\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(glove_input_file, word2vec_output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s7so6mkXr8b"
      },
      "source": [
        "You now have a copy of the GloVe model in word2vec format with the filename glove.6B.100d.txt.word2vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGlpG4YLXx6c"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# load the Stanford GloVe model\n",
        "filename = 'glove.6B.100d.txt.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
        "# calculate: (king - man) + woman = ?\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}